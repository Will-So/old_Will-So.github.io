{"pages":[{"url":"pages/about.html","text":"Analytical Sense is a technical blog for Data Science and coding. Here I post overviews of projects I am working on, Data Science insights I believe would be useful for others, as well as related topics. I'm Will, a Data Scientist in New York. I lead Data Science at Octane Lending. My favorite tools are Python-centric (PyData, Dask, PySpark, etc.) and I am also a big fan of Shiny and the Tidyverse. I can be contacted at wil.sorenson@gmail.com. My Linkedin can be found here","tags":"pages","title":"About"},{"url":"pages/projects.html","text":"Here are some interesting data projects that I have worked on. Data Projects Maximizing Returns in Peer to Peer Lending. Blog Post . Github Finding the best place to start a restaurant. Github . Blog Post Making wildly accurate sentiment analysis. Github Finding the best remakes and sequels in the Film Industry. Blog Post . Github .","tags":"pages","title":"Projects"},{"url":"pages/glossary.html","text":"The variety of terminology in Predictive Analytics/Data Science may be without rival. Even a simple idea like \"feature\" (of a predictive model) has the following names: 1) Feature, 2) Independent Variable, 3) Regressor, 4) covariate, 5) predictor variable, 6) Factor. This page is an attempt to make the jargon of the field less nebulous. Sizes of Data Definitions of small, medium, and big data vary wildly. Here are the best answers I have been able to come up with: Size Description Small Data can be loaded in the memory of a new laptop and relatively memory-intensive operations (e.g., SVM) can be run on that laptop. In practice, this means that the Dataset is \\( < 2-4 GB\\) . Medium.1 Data can be loaded on the memory of an Amazon VPS (244GB ram) and relatively memory-intensive operations can be run on that laptop. Datasets bigger than small data sets but less than 15-30GB fall into this category. Medium.2 Data will fit on a hard drive and out-of-core operations (e.g., those offered by Vowpow Wabbit) can be completed in a reasonable amount of time. Data bigger than small but smaller than the size of a normal hard drive fall into this category. Big Clustered computing (e.g., Spark, Hadoop Map Reduce) is required to work with this data. The relevance of Models Term Description External Valditiy The degree to which a model or experiment is valid outside of its native setting. A model with good external validity will predict observations well that it did not see during its development (training). Experimental results with good external validity produce valid predictions or lessons in the real world. Confusion Matrix Describes number of True Positives/True Negatives/False Positives/False Negatives that a model predicts Source: Yhat True/False Negative/Positive This table is similar to the one above but shows the table of true and false negatives Precision Fraction of identified positives that really are positive. Recall \\(True\\_Positives / (True\\_Positives + False\\_Negatives)\\) Proportion of correct positives identified. Sensitivity Same as Recall Specificity Proportion of correct negatives identified. False Positive Rate \\( \\frac{FP}{FP + TN}\\) Ideal value is 0. Statistical Modeling Term Definition Dummy Variable Also called dichotomus, indicator, boolean, or binary variables. A variable that is a 0 or 1 depending Nearest Neighbor Search (NNS) A family of algorithms that find the closest points. K-d tree and linear search are the two most common flavors of this. Cloud Terms Term Definition EC2 Virtual Private Server from AWS. AWS Amazon Web Services. The umbrella term for the cloud services that Amazon offers. EMR Hadoop/Spark on AWS. VPS Virtual Private Server Remote Host The computer in the cloud that you are somehow controlling. Local Host The computer that is physically in front of you. AMI Essentially configuration instructions for an EC2 instance. Tells the server which OS to load and which software to load. We can easily make our own with software of our choosing. Security Group EC2 profiles that specify which computers and which protocols may connect to our EC2 instance. SSH Secure Shell; allows you to log in to a remote host. SCP Secure Copy; allows you to copy files to a remote host. References Vowpow Wabbit Speed On Precision, Recall, etc. terminology if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"pages","title":"Glossary"},{"url":"Dealing_Wi.html","text":"Making Models Robust to Outliers One of the more difficult part of making predictive models is making sure that our model is robust to outliers. Fitting data strongly to outliers tends to give us bad predictive results. There are two solution that will work well in most cases and the are a few other methods worth considering when the first two either fail or are impracticle. Use a robust loss function Mean squared error (MSE) and thus root mean squared error (RMSE) are very sensitive to outliers ( 1 ). Sometimes, this is desirable where we want to punish an error of 20 much more than an error of 5. (400 vs 25). This is not desirable when we are trying to avoid outliers from having an exaggerated effect on our model. Mean Absolute Error is somewhat less sensitive to outliers because it does not square the errors. It is not \"robust\" to outliers but it represents a middle ground between traditional loss functions and methods discussed below. The Huber Loss function One of the better loss functions to choose is the Huber Loss function. It is defined as: $$L_\\delta (a) = \\begin{cases} \\frac{1}{2}{a&#94;2} & \\text{for } |a| \\le \\delta, \\\\ \\delta (|a| - \\frac{1}{2}\\delta), & \\text{otherwise.} \\end{cases}$$ This has the effect of combining Squared Errors and Absolute Errors. For $|a| \\le \\delta$, the error function represents the RMSE. For $|a| \\ge \\delta$, the error function represents Mean Absolute Errors. Winsorizing the Data Winsorizing the data provides the same results as the Huber Loss Function when $\\delta$ in the huber loss function is equal to the point at which the value is winsorized. This has a large disadvantage compared to the Huber Loss Function in that we are making changes to the dataset. That leaves us with a choice between making two copies of the data (one winsorized and non-winsorized) or discarding some information that may come in useful in future analysis. The former may not be practical for very large datasets. An advantage of winsorizing data is that it makes it easy to visualize that data in a meaningful way. Pandas has pd.clip() for this capability which makes it easy to use. Source: Wikipedia Discarding Outliers Most data munging has outliers that are almost certainly products of measurement errors or very rare anomalies. Some examples of this: Demographic surveys where someone has 17 PhDs Someone borrowing from lending club with a yearly income of 7M. A stock transaction with a trade time before stock markets opened. Even in the event some of those observations were accurately measures (quite unlikely), they almost surely lack external validity so will only hurt the performance of our model unless we delete them. Use a Model that is Robust to Standard Errors Support Vector Machines in particular tend to not consider outliers during training. Gradient Boosting, and Random Forests also have nice properties that make it difficult (but not impossible) to overfit a training dataset. A Bad method: Log transforms Log transforms are often mooted as a good way to make outliers stand out less. When we use a log transform we are allowing the outliers to dictate how we describe all of our observations . This is just the opposite of using a robust measure. This is not to say that log transforms should not be used. There are many situations when log transforms should be used due to the behavior of all the observations : When the errors have an extreme positvely skewed distribution. We then use the Box-Cox . This allows us to improve the validity of the Pearson Correlation coefficient, stabalize variance, and in general make traditional descriptive statistics more meaningful. When the relationship being investigates is close to exponential. When you want to represent a model that explains the explanatory variables' influence in terms of percentages (relative values) rather than absolute differences. Footnotes [1]: This is easy to see from the mathematical definition of MSE: $1/n \\sum( \\hat Y_i- Y_i)&#94;2$. Since the distance between the sample mean and the datapoint is squared, it exaggerates the influence of outliers. References http://stats.stackexchange.com/questions/48267/mean-absolute-error-or-root-mean-squared-error http://projecteuclid.org/download/pdf_1/euclid.aoms/1177703732","tags":"","title":"Dealing With Outliers: The Concepts"},{"url":"Starting_a.html","text":"Founding a successful restaurant is hard. 60% fail in 3 years. This pales in comparison to hotel chains that average a 7% failure rate over 10 years. Growing up, my uncle Jim struggled with his restaurant for 3 years before he finally admitted defeat. Can we use Data to improve the chances of starting a successful restaurant? This month, I decided to find out. The goal for this project is: Find the location that maximizes their chances for success Choose the ameneties and other features that maximize their chance of success Predict their chances of success given their prospective location and amenities. 1. Getting the Data Thanks to the Yelp Challenge, the data I needed was easy to get and the data was shockingly consistent. There were next to no anomalous data points. 2. Defining Success One challenge of this project is defining success. I don't have the profits and losses of individual restaurants so I need to infer success based on observable data we have. I define the function as $F(Rating, \\frac{Number\\_Reviews}{Month}, Time)$. The actual form of this function is somewhat complicated and can be seen in the measuring_success notebook. The top 25% of Yelp Restaurants according to this function were labeled as successful. 3. Feature Engineering Location Location Location One of the key features in my model is the success of businesses in a nearby location. I use a K-d tree to efficiently find the 10 nearest businesses to a given point and then average those success values. Latent Topics I use latent topics with Latent Dirichlet allocation to make features. This combination of using an unsupervised learning technique to generate features for a supervised model can provide powerful insights. Unfortunately, latent topics in this case were simply not predictive of success. This is despite coherent categories. Explicit Topics Explicit topics (i.e., categories) perform much better than LDA topics. They end up predicting nearly as well as location in the aggregate. 4. Modeling, Evaluating, and Refining I tried a number of different classifiers at first. Random Forests performed the best but there was still something wrong with it. It had way too many false positives. If my model predicted that the restaurant would be successful, the restaurant would only actually be successful 50% of the time. This is much better than change (25%) but I thought we can do better. I adjusted the classifer to have balanced classes in each tree and this decreased the false positive substantially. Now my model had a true positive rate of 66%. This represents more than a 150% increase in the chanse of being successful. The most important features: 5. Building the Web App Since most people who want to start restaurants tend not to be very technically savy, I decided to build a web app that allows people to easily use the model I built. Building the app was extremely straightfoward thanks to the awesomeness of Flask. This is a screenshot of the app I built: The user workflow is: User picks city. Map pans to city. User Picks the type of restaurant User clicks on the map to specify a location App returns probability of success. Conclusion Using data to inform our decisions when founding a new business can be extremely valuable. This project also shows how valuable a good frontend can be for communicating insights in data. Without a good interface it would be difficult for people to make use of this data. Threats to Model Validity Maturation: The test dataset that we evaluated accuracy on only has currently existing companies. It could be the case that past patterns of success are no longer predictive. Saturation: My model does not consider the potential of saturation. If we have too many sushi places right next to eachother, this can cause problems for profitability. Businesses being close to eachother is a good strategy to a point The Next Iteration Add a heat map The app is currently not hosted anywhere. I hope to deploy it in the near future so others can make use of it. Add NLP features based on the reviews of the restaurant. Generalize to other businesses.","tags":"","title":"Starting a Successful Restaurant"},{"url":"Predicting.html","text":"One of my latest projects has beenPredicts the default rate of loans and automatically order the model's favorite currently available loans. Likely to increase ROI from 7.5% (Lending Club Average) to 13.5%. This project identifies investment stratagies that fit custom investment profiles. The user specifies the amount of capital she is willing to invest and either her desired expected return or her maximum risk tolerance. Key Features: Choosing the model's favorite loans (top 20%) leads to an expected ROI of 13.5% Provides a web app to view the currently available loans with their predicted ROI and default probability. Use the lending club API to order the model's favorite loans automatically. An Example of viewing currently available loans: The github repository can be found here . Some Descriptive Statistics FICO scores do an excellent job predicting the default rate of P2P borrowers: People with incomes $<70k$ are more likely to default than richer borrowers: Those who pay a very large percentage of their income to debt are more likely to default: Model Selection I tried a number of different machine learning models for this project. Of them, Random Forests ended up performing the best. Details can be found in modeling.ipynb and modeling.ipynb II . Feature Engineering I tried a few custom-made features in my model. Notably, the ratio of total debt payments (lending club and other outstanding debt) to income was my second best feature. It is somewhat surprising that this model performs so well because it does not seem particurlaly important in the Descriptive Statistics section. Results If we pick my model's favorite 25% of loans, our default rate will be half that of similarly-graded notes. This represents an increase from 9% (ROI for C-G notes) to 13.5%. Turning our Results into a Product I made a very simple web-app to display the currently available notes and their expected ROI and default probability. I also made a script that will automatically order the best currently-available notes in order of expected ROI. I then told cron to run it 4x a day (when the notes are available). That way I an investment strategy that requires 0 effort from this day forward. Both of these can be found in the app directory. The Next Iteration Make the Web App much better Integrate with Tableau Dashboard Try some more ML models to see what the best results will be. Especially gradienting boosting. Clean up the HTML files so that they are easier to read. Try some new ML Models Gradient Boosting Machines Allow multiprofile support A lot of LC power users have multiple accounts. Make more tests","tags":"","title":"Predicting Lending Club Defaults for Fun and Profit"},{"url":"Everything.html","text":"Random Forests are an increasingly popular machine learning algorithim. They perform well in a wide variety of learning and prediction problems despite being exceedingly easy to implement. Random forests can be used for both classification and regression problems. This post describes the intuition of how Random Forests work as well as its. advantages and disadvantages. Intuition for How Random Forests Work At a high level, the algorithim usually works as follows: Randomly sample $N$ observations from the training set at random with replacement [1] Train a decision tree using $N$ and $\\sqrt{num\\_features}$. Repeat (1) and (2) B times, where B is the number of trees (also called bags). Depending on the nature of data, the ideal number of trees ranging from 100 to several thousand. Source: Jan Vitek, Perdue Main Advantages Embarrassingly Parallel SKlearn will allow you to use all the cores in your computer to train multiple trees at the same time. The algorithim is also available in Spark. This is because all random forests are trained independently of eachother. Resistant to Overfitting The only way to overfit the model is to have trees that have too many branches. Increasing the number of trees does not increase the risk of overfitting. Instead an increasing number of trees tends to decrease the amount of overfitting No Need for Cross Validation We can also avoid cross-validation by using the out of bag score . This is available as an argument in Spark and Sklearn and provides nearly identical results to N-fold cross-validation.[2] Other Advantages Most implementations make it trivial to find feature importance It is very easy to Tune Random Forests excel with highly non-linear relationships in the data. Main Disadvantages Can perform worse than Gradient Boost Gradient Boosted Machines tend to perform better under both of the following conditions: We correctly tune the relatively complicated hyperparamters of GBM. Either 1) your data fits in ram (for sklearn) or 2) your problem is not a multiclass classification problem (for Spark). (My understanding, not certain) Non-linear relationships Slow to Predict Since predicting a new observation requires running the observation through every tree, Random Forests will often perform too poorly for real-time prediction. Doesn't behave well for very sparse feature sets In this case, SVM and Naive Bayes tend to perform better than Random Forests. One of the reasons for this is because each tree only has access to $\\sqrt{n}$ features by default. If very few features are of any importance, most trees will miss important features. Using it in sklearn Below is an example of using a Random Forest with 50 trees: In [ ]: model_rf = RandomForestClassifier ( n_estimators = 200 , oob_score = True , verbose = 1 , random_state = 2143 , min_samples_split = 50 , n_jobs =- 1 ) fitted_model = model_rf . fit ( X , y ) The oob_score argument tells the classifier to return our out of sample (bag) error estimates. The min_samples_split=50 argument tells the classifer to only create another branch of the tree if the current branch has more than 50 observations. n_jobs=-1 tells SKlearn to use all the cores on my machine. Using it in Spark An example of using Random Forests in Spark can be found here . Here is the same example as in SKlearn: Practical tips A good way to prevent overfitting is to set a relatively high tree-splitting threshhold. This is the min_samples_split argument in SKLearn. We can easily check if we are overfitting by looking at the out of bag error. More trees are always better than fewer. The returns start to dominish quickly. Source here Having trees that are too deep can lead to overfitting. This can be avoided by increasing the number of trees. The default number of features ($\\sqrt{n}$) is usually a sufficiently good value. After picking the best Random Forest with your out of bag errors, Footnotes [1]: This means that the sample size will be the same as the training set size but the composition of the sample will be different because it is being sampled with replacement. [2]: This works by looking at the errors of the trees where the relevant observation $(X_i, y_i)$ did not occur. References Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001. APA http://www.slideshare.net/0xdata/jan-vitek-distributedrandomforest522013 http://scikit-learn.org/stable/modules/ensemble.html Random Forests by Leo Breiman","tags":"","title":"Everything You Need to Know to Use Random Forests"},{"url":"Jupyter_on.html","text":"Using the Jupyter Notebook with an EC2 instance allows us to analyze data of a size that would be difficult or impossible to do on our local machines. The most powerful (from a memory perspective) EC2 instance has 244GB of Ram and 32 cores and often costs only 50 cents/hr. This tutorial explains: How to Set up the Jupyter Notebook on an EC2 instance so we can use it from a web browser. How to keep the notebook running even if the SSH session closes. A workflow for dealing with spot instances. This post assumes you are already familiar with the basics of EC2. You should know how to start an instance and SSH into it. Step 1: Setting up your EC2 Instance Go to the EC2 Management Console and select launch instances Choose which AMI you want to use. I recommend Ubuntu. There might be some minor differences if you choose something else. [2] Select which type of instance you want and change any additional settings In the security group page, enable HTTPS and Port 9999. Port 9999 is the port we will be accessing the notebook from: Connect to your EC2 Instance with SSH Install all the software you need including the Notebook[3] The link to install conda can be found here . Just copy the link address for linux and then use wget link on your remote host to download it. Just follow the directions on the page. The installation will ask whether to append the conda environment to the Python path. Type in yes After installing, you need to reload .bashrc (type . .bashrc ) Step 2: Configuring the Notebook Server The Jupyter Documentation provides an excellent walkthrough on configuring the server here . The tl;dr is: In the terminal type: jupyter notebook --generate-config in order to generate the configuration file. In IPython type in from notebook.auth import passwd; passwd() and then enter in the password you want to use to access the notebook. Remember the password and copy the hash you get back. Generate an SSL certificate using the following code: openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.key -out mycert.pem Open the configuration file ( ~/.jupyter/jupyter_notebook_config.py by default) you made with your favorite text editor and copy in the hash generated from passwd(): c = get_config () c . NotebookApp . password = u 'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed' c . NotebookApp . ip = '*' c . NotebookApp . open_browser = False c . NotebookApp . port = 9999 c . NotebookApp . certfile = u '/home/ubuntu/mycert.pem' c . NotebookApp . keyfile = u '/home/ubuntu/mycert.key' In .bashrc make the following alias: alias jupyter='nohup jupyter notebook &' Then reload .bashrc again. This also makes your jupyter session persistent. It will not stop running just because the ssh session is closed. Now you can type jupyter in the terminal and the server should run. Then browse to https://ip_address:9999 and login with the password you entered: Protecting your data while using Spot Instances Spot instances are much cheaper than on-demand instances. They downside is that you can be kicked off your instance if the amount of capacity of the Spot Instance gets too high. The best way to deal with the transient nature of spot instances is to use the following workflow: Write the code on your local-host Prototype the code on your local-host with a small sample of the data you want to process. Move your code to your EC2 instance and then run the code. I usually use github for this. This way we minimize the amount of time that we have to run the EC2 instance and we don't have to worry about losing any code because it is all in the local host. If you end up writing novel code in your EC2 instance I recommend keeping your code synced with github. Footnotes [1] Some commands might be different above if you choose a different AMI. [2] In the future, I recommend creating your own AMI so you don't have to do this every time. [3] For installing the apps, I recommend you use Anaconda to install Python and dependencies. Note that you will have to restart your shell session in order to use","tags":"","title":"Jupyter on the Cloud -- A How-To"},{"url":"films_2015.html","text":"Over the past couple of years Netflix and other tech firms have been enormously successful producing TV series for their subscribers. What would this strategy look like in the case of movies? I use data publicly available from Box Office Mojo and IMDB to find the films that are most likely to have a successful sequel. Movies vs TV Series For subscription services like HBO and Netflix, focusing on series rather than movies makes intuitive sense. This is because both series and subscription services are recurring on a regular basis. Despite this intuition, history suggests that people are willing to subscribe for movies as well. HBO relied soley on movies for the first 20 years of its existence. Producing original content can also make a lot of sense. An expensive series like House of Cards costs about 60 million a season. Given that Netflix has 33.3 Million subscribers . Since new subscribers are going to be paying \\$120 a year, Netflix only has to increase the number of their subscribers by 1.5% to break even. In the case of house of cards, it seems likely that Netflix made a good profit on the series. In [2]: costs = 50e6 ; subsciber_cost = 120 sub_rev = subsciber_cost * 33.3e6 print ( \"$ {:,} \" . format ( int ( sub_rev * 0.015 ))) $59,940,000 The breakeven point for movies can be much lower. Summer blockbusters budgets may attract the most attention with their eyewatering budgets but many movies cost 25 million or less. The break-even point for a 25MM film would be to increase the subscriber base by about 0.7%. Stylized Facts My investigation yields the following relevant results: The single most important factor is to be beloved by the audience. 1% increase in rating of the masses associated with a 6% increase of ROI. critics unimportant. Though Budget does a great job of predicting box office sales, it negatively correlated with ROI. A 10% increase in production budget is associated with a 5% decrease in ROI. High-cost films are much more unpredictable than than low-cost films Audience reviews of the original does not predict reviews of the first Month of year is unimportant once controlling for the above except for October. Sequels have a high ROI. Gross on average 5 times their production budgets Under traditional assumptions: 278% ROI Under realistic assumptions: 112% ROI Strategy Finding the most promising remake candidates requires understanding the movie industry better. I use ROI to measure the success of a movie because it is an excellent proxy for how many people viewed the movie. I train a linear regression model that predicts ROI based on a basket of features retrieved from IMDB and Box Office Mojo. I divide my dataset of sequels into a training set (85% of total data) and a test set (15%) of total data. I then use a linear regression with L1 normalization to find the most important factors that are correlated with ROI. My model assumes that there are no confounders ($E[\\epsilon \\mid X] = 0$. This is a major limitation of my model. Important Relationships Everything else is not significant More or less every regression looks like Important Features I find that only budget, audience rating, and being realased in October have a statistically significant effect on ROI. Surprisingly, the quality or box office of the original film does not seem to influence the results. My analysis suggests that it is difficult to predict ex-ante the success of movies. Factor Change in ROI Released in Month of October $\\Uparrow 5\\%$ $\\Uparrow$ Budget 10% $\\Downarrow 5.2\\%$ $\\Uparrow$ Audience Rating 1% $\\Uparrow$ 6.3% Other months 0* Runtime 0* Genre 0* Days since sequel 0* Months other than October 0* MPAA Rating 0* Critic rating 0* Change in PB from original 0* Audience rating of original 0* * Statistically insignificantly different from 0 What Kinds of Movies Should be Remade? Lower risk: Focus on cheaper films Less likely to have massively negative ROI. Able to produce many cheap films rather than a few expensive films Release the Movie in October Unmet demand before the busy months of November/December? Do a good job on the sequel Audience opinion on sequels not correlated with the rating of the original film. Keep it short Runtime not associated with ROI. Do not take this logic too far. The Most Promising Sequels Keeping the above in mind, these are the films that have the best chance to suceed. As many films (~200) satisfy all 4 conditions above, films are ranked by their thumb-rule ROI. Gone with the wind Juno Crocodile Dundee Blair Witch Project The Rocky Horror Picture show National Lamboon's Animal House Platoon Fahrenheit 9/11 Magic Mike The Usual Suspects. Appendix See the appendix and accompanying code here . The appendix contains information like how I computed ROI as well as various limitations of the model and more descriptive statistics.","tags":"misc","title":"In Search of the Most Promising Movie Remakes or Sequels"},{"url":"Quickly_Make_CLI_Tools_with_Python.html","text":"Synopsis If thinking of making a CLI tool, consider using Python with click . It is very fast and will make your life easier. It is probably the most straightforward way to automate tasks. This script removes an enormous headache of making blog posts. As I already tend to take notes using the Jupyter Notebook, it makes it very easy to blog regularly. Which Library to Use There are a number of good libraries that make making CLI apps with Python easy. I am personally a big fan of click as it seems to be the quickest way to get running. I have a short CLI available that automates my blog posting workflow here . Here is the heart of pelican_autopost . This little code already makes all of the necessary documentation, prompts, default values, and arguments. In [3]: import click BLOG_DIR = '~/blog' @click . command () @click . option ( '--blog-dir' , default = BLOG_DIR ) @click . option ( '--notebook-path' , prompt = 'Enter the full notebook path' , help = 'files must be''located in `blog_dir`' ) @click . option ( '--title' , prompt = 'enter the title. Must be unique' , default = None ) @click . option ( '--tags' , prompt = 'enter a tag. You may enter more than one via' , multiple = True ) @click . option ( '--category' , default = None ) def _main ( blog_dir , notebook_path , title , tags , category ): \"\"\" Simple program that takes the directory of a notebook (use `greadlink -f file.ipynb | pbcopy`) and other parameters and then publishes it to my pelican blog. Examples ---- ./pelican_auto_post.py --tags *Nix --tags CLI --title sqlite3 Enter the full notebook path: /Users/Will/Devel/blog_posts/sqlite3_CLI.ipynb \"\"\" if not title : title = os . path . splitext ( notebook_path )[ 0 ] make_md ( blog_dir , title , tags , category , notebook_path ) copy_notebook ( notebook_path , blog_dir , title ) publish ( blog_dir ) Running the Script from anywhere Now that we have the script, we can easily run it from anywhere by performing the following steps: Make a directory for all of your CLI tools if you don't already have it. I use ~/scipts . Move your script to here. Add this directory to your PATH environmental variable. For OSX this is ~/.bash_profile Make sure you have the proper shebang line added to the top of your file #!/usr/bin/env python3 Executing Bash Scripts Executing bash scripts from Python can be tricky. The Github describes how I did it but the most robust way to do this seems to be to execute the data directly from python like this: In [ ]: from subprocess import call with open ( '~/scripts/script_name.sh' ): script = file . read () rc = call ( script , shell = True ) Make sure that you only run this with trusted code that oyu are feeding it yourself. Otherwise it poses a security issue. An alterantive option would be to santize the file in Python before using subprocess.call on it. Testing If the CLI is quite small, often times the best test is just going to be the script itself. I recommend to only test to prevent the things that can cause problems (accidently overwriting or deleting files). In the pelican autopost tool, for example, the only test I feel helped is the one that insures that the markdown files won't be overwritten because this could cause problems if there were to be two blog posts of the same title.","tags":"","title":"Quickly Make CLI Tools with Python"},{"url":"sqlite3.html","text":"What is sqlite3 good for? Using the sqlite3 CLI can often be a good alternative to python-based solutions when the operations we are performing is easy to express directly with SQL. How does it work? Commands like .tables are prefixed with a dot. SQL statements select * from AllstarFull end with a semicolon Useful commands are: .tables : Shows all tables in the db. .timer ON : times how long an operation takes between .timer ON and .timer OFF .schema shows the schema of all the tables in the db Cookbook We can dump files to csv via the following syntax . mode CSV . headers on . output table . csv We can run an SQL script from the command line using the following syntax sqlite3 database.db < sql_script.sql After dropping a table, you may have to use VACUUM; to reclaim the space that the dropped table was using. Dealing with Database or Disk is Full When dealing with tables that are large compared to the space on disk it's possible to get an error that says: Error: near line 41: database or disk is full even though the disk isn't full and even though the database is far smaller than the 120TB size limit. This could be because either 1) the temporary file directory is filling up or 2) the database is corrupted. In the case of (1), the solution is either to tell SQLite to put everything in ram via sqlite> prgrama temp_store = 2; . If you don't have enough ran for the operation, you can change the temporary directory with the following code: sqlite> pragma temp_store = 1; sqlite> pragma temp_store_directory = '/spacious/hdd/data'; In the case of (2) hope is still not lost. Search the internet for ways to repair a corrupt SQLite database. When to use SQLITE SQLite is much easier to work with than other RBDMS because it is serverless, requires few external libraries, and requires no configuration. I think of SQLite as being the Python of databases. If you are using an RBDMS for a project, you should have a good reason not to use SQLite. On websites it can handle 100k hits/day with ease and 500k is probably a more realistic upper bound . The SQLite website offers a few cases when SQLite probably is not the best choice. If your DB is so big that it can't fit on a single disk, then you probably need something else. Websites with very high write volumes Multiple uers need to write to a db simultaneously. You aren't storing your RBDMS on the same hard drive as your app.","tags":"","title":"sqlite3"},{"url":"WCCLI.html","text":"What is WC good for? wc is available by default on my *nix machines. It is useful to determine the following about a file: The number of words or characters in a file. The number of columns in a file the number of lines in a file Examples wc -l newdataset.csv Returns the number of lines in newdataset.csv wc -w masterthesis.txt returns the number of words head -1 newdataset.csv | wc -w returns the number of columns in a file IF every column name is only 1 word. This is usually the case with csv files and virtually always the case with tsv files. Run just the first command first to verify that this is the case.","tags":"misc","title":"WC CLI Quick Reference Part 1"},{"url":"license.html","text":"The MIT License (MIT) Copyright (c) 2014 Daan Debie Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","tags":"misc","title":"license"}]}